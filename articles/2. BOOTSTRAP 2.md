# Bootstrapping a New Project - Part 2

In my [previous article](#todo link), I've demonstrated how to set up a project from scratch, 
create fixtures and get the project up and running. 
Next step on our journey is populating the database with a somewhat realistic amount of data 
to test app performance. 

Additionally, I will demonstrate how to setup simple PHPUnit test suite, and Gitlab based CI pipeline.

## More Fake Data

Once your entities are polished, and you had "That's it! I'm done!" moment, it's a perfect time for 
creating a more significant dataset that can be used in further testing and preparing your app for production.

Simple fixtures as ones we created in the previous article are great for the development phase where 
loading ~30 entities is done quickly and we can repeat it often while changing the DB schema.

Testing app performance, simulating real-world traffic and detecting bottlenecks requires bigger dataset
(i.e., larger amoung of database entries and image files for this project). 
Generating few thousands of entries takes some time (and computer resources), so we want to do it only once.

We could try to increase the `COUNT` constant in our fixture classes and see what will happen:

```
// src/DataFixtures/ORM/LoadUsersData.php
class LoadUsersData extends AbstractFixture implements ContainerAwareInterface, OrderedFixtureInterface
{
    const COUNT = 500;
    ...
}

// src/DataFixtures/ORM/LoadGalleriesData.php
class LoadGalleriesData extends AbstractFixture implements ContainerAwareInterface, OrderedFixtureInterface
{
    const COUNT = 1000;
    ...
}

```

Now if we run `./bin/refreshDb.sh`, after some time of execution we'll get a not-so-nice message `PHP Fatal error:  Allowed memory size of N bytes exhausted`. 

Apart of slow execution, every error would result in an empty database because EntityManager is flushed only at the very end of the fixture class.
Additionally, Faker is downloading a random image for every gallery entry. 
For 1.000 galleries with 5 to 10 images per gallery that would be 5.000 - 10.000 downloads which is really slow.

There are excellent resources on optimizing [Doctrine](http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/batch-processing.html) and [PHP](https://groups.google.com/forum/#!topic/symfony-devs/0Ez-TpsC3I0) 
for batch processing, and I'm going to use some of these tips to optimize fixtures.

First, I'll define a batch size of 100 galleries. 
After every batch, I'll flush and clear the `EntityManager` (i.e., detach persisted entities) and tell garbage collector to do its job.  

To track progress, I will print out some meta informations (batch identifier and memory usage).

**Note:** After calling `$manager->clear()` all persisted entities are now unmanaged. 
Entity manager don't know about them anymore and you'll probably get some entity-not-persisted error. 
The key is to merge entity back to the manager `$entity = $manager->merge($entity);`

Without the optimization, memory usage is increasing while running a `LoadGalleriesData` fixture class:
```
  > loading [200] App\DataFixtures\ORM\LoadGalleriesData
100 Memory usage (currently) 24MB / (max) 24MB
200 Memory usage (currently) 26MB / (max) 26MB
300 Memory usage (currently) 28MB / (max) 28MB
400 Memory usage (currently) 30MB / (max) 30MB
500 Memory usage (currently) 32MB / (max) 32MB
600 Memory usage (currently) 34MB / (max) 34MB
700 Memory usage (currently) 36MB / (max) 36MB
800 Memory usage (currently) 38MB / (max) 38MB
900 Memory usage (currently) 40MB / (max) 40MB
1000 Memory usage (currently) 42MB / (max) 42MB
```

Memory usage starts at 24 MB and increases for 2 MB for every batch (100 galleries).
If we tried to load 100.000 galleries, we'd need 24 MB + 999 (999 batches of 100 galleries, 99900 galleries) * 2 MB = **~2 GB of memory**.

After adding `$manager->flush()` and `gc_collect_cycles()` for every batch, removing 
SQL logging with `$manager->getConnection()->getConfiguration()->setSQLLogger(null)` and
removing entity references by commenting out `$this->addReference('gallery' . $i, $gallery);`, 
memory usage became somewhat linear for every batch.

```
// We define batch size outside of the for loop
$batchSize = 100;

...

for ($i = 1; $i <= self::COUNT; $i++) {
    ...
    
    // Save the batch at the end of the for loop
    if (($i % $batchSize) == 0 || $i == self::COUNT) {
        $currentMemoryUsage = round(memory_get_usage(true) / 1024);
        $maxMemoryUsage = round(memory_get_peak_usage(true) / 1024);
        echo sprintf("%s Memory usage (currently) %dKB/ (max) %dKB \n", $i, $currentMemoryUsage, $maxMemoryUsage);
    
        $manager->flush();
        $manager->clear();
    
        // here you should merge entities you're re-using with the $manager
        // because they aren't managed anymore after calling $manager->clear();
        // e.g. if you've already loaded category or tag entities
        // $category = $manager->merge($category);
    
        gc_collect_cycles();
    }
}

```

We can see the difference:

```
  > loading [200] App\DataFixtures\ORM\LoadGalleriesData
100 Memory usage (currently) 24MB / (max) 24MB
200 Memory usage (currently) 26MB / (max) 28MB
300 Memory usage (currently) 26MB / (max) 28MB
400 Memory usage (currently) 26MB / (max) 28MB
500 Memory usage (currently) 26MB / (max) 28MB
600 Memory usage (currently) 26MB / (max) 28MB
700 Memory usage (currently) 26MB / (max) 28MB
800 Memory usage (currently) 26MB / (max) 28MB
900 Memory usage (currently) 26MB / (max) 28MB
1000 Memory usage (currently) 26MB / (max) 28MB
```

Instead of downloading random images every time we can prepare 15 random images and update fixture script to
use it instead of using Faker's `$faker->image()` method. 

I'm going to take 15 images from [Unsplash](https://unsplash.com) and save them in `var/demo-data/sample-images`.
I will update the `LoadGalleriesData::generateRandomImage` method to use existing images instead of 
downloading new ones:

```
private function generateRandomImage($imageName)
    {
        $images = [
            'image1.jpeg',
            'image10.jpeg',
            'image11.jpeg',
            'image12.jpg',
            'image13.jpeg',
            'image14.jpeg',
            'image15.jpeg',
            'image2.jpeg',
            'image3.jpeg',
            'image4.jpeg',
            'image5.jpeg',
            'image6.jpeg',
            'image7.jpeg',
            'image8.jpeg',
            'image9.jpeg',
        ];

        $sourceDirectory = $this->container->getParameter('kernel.project_dir') . '/var/demo-data/sample-images/';
        $targetDirectory = $this->container->getParameter('kernel.project_dir') . '/var/uploads/';

        $randomImage = $images[rand(0, count($images) - 1)];
        $randomImageSourceFilePath = $sourceDirectory . $randomImage;
        $randomImageExtension = explode('.', $randomImage)[1];
        $targetImageFilename = sha1(microtime() . rand()) . '.' . $randomImageExtension;
        copy($randomImageSourceFilePath, $targetDirectory . $targetImageFilename);

        $image = new Image(
            Uuid::getFactory()->uuid4(),
            $randomImage,
            $targetImageFilename
        );

        return $image;
    }
``` 
 
It's a good idea to remove old files in `var/uploads` when reloading fixtures so I'm adding `rm var/uploads/*` command to `bin/refreshDb.sh` script,
immediately after dropping the DB schema.

Loading 500 users and 1000 galleries now takes ~7 minutes and ~28 MB of memory (peak usage).

```
Dropping database schema...
Database schema dropped successfully!
ATTENTION: This operation should not be executed in a production environment.

Creating database schema...
Database schema created successfully!
  > purging database
  > loading [100] App\DataFixtures\ORM\LoadUsersData
300 Memory usage (currently) 10MB / (max) 10MB
500 Memory usage (currently) 12MB / (max) 12MB
  > loading [200] App\DataFixtures\ORM\LoadGalleriesData
100 Memory usage (currently) 24MB / (max) 26MB
200 Memory usage (currently) 26MB / (max) 28MB
300 Memory usage (currently) 26MB / (max) 28MB
400 Memory usage (currently) 26MB / (max) 28MB
500 Memory usage (currently) 26MB / (max) 28MB
600 Memory usage (currently) 26MB / (max) 28MB
700 Memory usage (currently) 26MB / (max) 28MB
800 Memory usage (currently) 26MB / (max) 28MB
900 Memory usage (currently) 26MB / (max) 28MB
1000 Memory usage (currently) 26MB / (max) 28MB
```

## Performance

Homepage rendering became very slow, way too slow for production. 
The user can feel that app is struggling to deliver the page, probably because we're 
rendering all galleries instead of a limited number.

Instead of rendering all galleries at once we will render first 12 galleries immediately and introduce lazy load.
When the user scrolls to the end of screen, the app will fetch next 12 galleries and present them to the user.

### Performance tests
In order to track performance optimization, we need to establish fixed set of tests we'll use to test and benchmark 
performance improvements relatively.

We'll be using Siege for load testing. Here you can find more about [Siege and performance testing](https://www.sitepoint.com/web-app-performance-testing-siege-plan-test-learn/). 

#### Testing The Homepage

Testing the homepage is not trivial since we have AJAX requests happening
only when users scroll to the end of page.

We expect all users to land on the homepage (i.e. 100%) and  
we guess that 50% of them would scroll down to the end and therefore request second page of galleries.
Furthermore, we guess that 30% of them would load 3rd page, 
15% would request 4th page and
5% would request 5th page.

These numbers are based on guess and predictions, it would be much better if we could take a
look at analytics tool and get better insight in our users' behavior, but that's impossible for a brand new app.

We'll test the homepage (and lazy load URLs) with two tests running in parallel.
First one will be testing the homepage URL onyl, while other one will test
lazy load URLs we will prepare so it fits our predictions.

File `lazy-load-urls.txt` contains randomized list of lazy loaded pages URLs 
in predicted ratios:

- 10 URLs for the 2nd page (50%)
- 6 URLs for 3rd page (30%)
- 3 URLs for 4th page (15%)
- 1 URLs for 5th page (5%)

```
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=4
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=3
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=4
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=4
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=3
http://blog.app/galleries-lazy-load?page=3
http://blog.app/galleries-lazy-load?page=3
http://blog.app/galleries-lazy-load?page=5
http://blog.app/galleries-lazy-load?page=3
http://blog.app/galleries-lazy-load?page=2
http://blog.app/galleries-lazy-load?page=3
```

Script for testing homepage performance will run 2 Siege processes in parallel, 
one against homepage and other one against generated list of URLs.

To run a single request via Siege (in Docker), execute:
```
docker run --rm -t yokogawa/siege -c1 -r1 blog.app
```

To run a 1-minute test with 50 concurrent users against homepage with 1 second delay, execute:
```
docker run --rm -t yokogawa/siege -d1 -c50 -t1M http://blog.app
```

To run a 1-minute test with 50 concurrent users against URLs in `lazy-load-urls.txt`, execute:
```
docker run --rm -v `pwd`:/var/siege:ro -t yokogawa/siege -i --file=/var/siege/lazy-load-urls.txt -d1 -c50 -t1M
```
from the directory where your `lazy-load-urls.txt` is located (that directory will be mounted as read-only volume in Docker).

Running a script `test-homepage.sh` will start 2 Siege processes (in a way suggested by this [StackOverflow answer](https://stackoverflow.com/a/5553774))
and output results from processes, something like this:

```
./test-homepage.sh
** SIEGE 3.0.5
** Preparing 2 concurrent users for battle.
The server is now under siege...** SIEGE 3.0.5
** Preparing 2 concurrent users for battle.
The server is now under siege...
Lifting the server siege...
Lifting the server siege...-      done.
      done.

Transactions:		          22 hits
Availability:		      100.00 %
Elapsed time:		       59.43 secs
Data transferred:	        0.06 MB
Response time:		        4.77 secs
Transaction rate:	        0.37 trans/sec
Throughput:		        0.00 MB/sec
Concurrency:		        1.77
Successful transactions:          22
Failed transactions:	           0
Longest transaction:	        8.15
Shortest transaction:	        2.54

FILE: /var/log/siege.log

Transactions:		          24 hits
Availability:		      100.00 %
Elapsed time:		       59.37 secs
Data transferred:	        0.30 MB
Response time:		        4.28 secs
Transaction rate:	        0.40 trans/sec
Throughput:		        0.00 MB/sec
Concurrency:		        1.73
Successful transactions:          24
Failed transactions:	           0
Longest transaction:	        8.15
Shortest transaction:	        0.90

FILE: /var/log/siege.log
```


#### Testing Single Gallery Page

Testing single gallery page is a little bit simpler - We need to run Siege against `galleries.txt` file
in which we have list of single gallery page URLs we want to test. 
Run the command:

```
docker run --rm -v `pwd`:/var/siege:ro -t yokogawa/siege -i --file=/var/siege/galleries.txt -d1 -c50 -t1M
```
from the directory where `galleries.txt` file is located (that directory will be mounted as read-only volume in Docker).

## Tests, tests, tests

In order to make sure we aren't breaking anything with our improvements, we need to have tests.

First we'll require PHPUnit as dev dependency: `composer req --dev phpunit`. 
Then we'll create simple PHPUnit configuration by copying `phpunit.xml.dist` file created by Flex to `phpunit.xml` and
update environment variables (e.g. `DATABASE_URL` variable for the test environment).
Also, we'll add `phpunit.xml` to `.gitignore`.

We'll add [functional/smoke tests](https://symfony.com/doc/current/best_practices/tests.html#functional-tests) for blog homepage and single gallery pages.
These tests would only assert that URLs we provide in `SmokeTest::urlProvider()` method 
are resulting with a successful HTTP response code (i.e., HTTP status code is 2xx or 3xx).

## Simple CI with Gitlab

todo

## Stay tuned

Upcoming articles in this series will cover details about PHP and MySQL performance optimization, improving 
overall performance perception and other tips and tricks for better app performance.
  